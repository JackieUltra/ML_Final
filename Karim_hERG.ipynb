{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "import PyTDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.single_pred import Tox\n",
    "data = Tox(name = 'hERG_Karim')\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data\n",
    "\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "# helper function to check SMILE strings from dataset w/ RDKit\n",
    "def validate_smiles(smiles):\n",
    "    try:\n",
    "        # converts SMILES to molecular obj\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # checks if molecular object is valid and returns true if it is\n",
    "        return mol is not None\n",
    "\n",
    "    #else false\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# helper function to clean the dataset splits\n",
    "def clean_data(split):\n",
    "    cleaned_split = {}\n",
    "\n",
    "\n",
    "    for key, df in split.items():\n",
    "        # removes dupes\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "        # fills in missing vals w/ median & unknown column\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype in ['float64', 'int64']:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "        # removes xtra space if there is any and converts binary column of Y into integers\n",
    "        df['Drug'] = df['Drug'].astype(str)\n",
    "        df['Drug'] = df['Drug'].str.strip()\n",
    "        df['Y'] = df['Y'].astype(int)\n",
    "\n",
    "        # checks SMILES strs and keep only valid ones that are in RDKit\n",
    "        df['Valid_SMILES'] = df['Drug'].apply(validate_smiles)\n",
    "        df = df[df['Valid_SMILES']]\n",
    "        df = df.drop(columns=['Valid_SMILES'])\n",
    "\n",
    "        #append cleaned data splits onto list\n",
    "        cleaned_split[key] = df\n",
    "\n",
    "    return cleaned_split\n",
    "\n",
    "#checking cleaned dataset splits\n",
    "cleaned_split = clean_data(split)\n",
    "print(cleaned_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance checker\n",
    "def check_balance(cleaned_split):\n",
    "    for key, df in cleaned_split.items():\n",
    "        print(f\"Split: {key}\")\n",
    "        y_distribution = df['Y'].value_counts(normalize=True)\n",
    "        print(y_distribution)\n",
    "        if (y_distribution.min() < 0.4) or (y_distribution.max() > 0.6):\n",
    "            print(\"unbalanced\")\n",
    "        else:\n",
    "            print(\"balanced\")\n",
    "\n",
    "check_balance(cleaned_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up test and train set + morgan fingerprints\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# create morgan fingerprints for SMILES using RDKit\n",
    "def generate_fingerprints(smiles):\n",
    "\n",
    "    # create molecule from smiles strings\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "      # radius of 2 to check interactions 2 bonds away, 2048 bits is\n",
    "      #(makes the ifngerprints 2048 bits), could use 1024 bits as well but it lowers f1 score\n",
    "      fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
    "\n",
    "      # turns fingerprint into a list of binary numbers\n",
    "      return list(fp)\n",
    "    else:\n",
    "      # no bits for hte ifngerprints if SMILES is invalid\n",
    "      return [0] * 2048\n",
    "\n",
    "# FEATURE ENGINEERING on training dataset\n",
    "train = cleaned_split['train']\n",
    "# create fingerprints for each molecules in the training dataset\n",
    "train_fingerprints = train['Drug'].apply(generate_fingerprints).to_list()\n",
    "# turn fingerprints into a df so the code can run\n",
    "X_train = pd.DataFrame(train_fingerprints)\n",
    "# retrieve the Y column of the dataset that has labels (blockers = 1, not a blocker = 0)\n",
    "y_train = train['Y']\n",
    "\n",
    "# same thing as above but on the test dataset\n",
    "test = cleaned_split['test']\n",
    "test_fingerprints = test['Drug'].apply(generate_fingerprints).to_list()\n",
    "X_test = pd.DataFrame(test_fingerprints)\n",
    "y_test = test['Y']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model\n",
    "# random state = 42 so we can run the code over and over again and get the same values\n",
    "# n_estimators = 500 which is the # of decision trees in the forest\n",
    "# we might still need to finetune the model with these paramters, but so far it's doing pretty good\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=500, )\n",
    "# train the model on the training dataset\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# model performnace printed out\n",
    "# class report says that model is doing pretty well\n",
    "print(\"Random Forest Classifier Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting (Baseline)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(random_state=42, n_estimators=300, learning_rate=0.01, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "print(\"Gradient Boosting Classifier Performance:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Accuracy: {accuracy_gb}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradinet Boosting  ->  better preformance (increased number trees and depth of trees)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(random_state=42, n_estimators=3000, learning_rate=0.01, max_depth=8)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "print(\"Gradient Boosting Classifier Performance Hypertuned:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Accuracy: {accuracy_gb}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# define the Gradient Boosting model with early stopping\n",
    "gb_model_early_stop = GradientBoostingClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=4000,  \n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    validation_fraction=0.2,  \n",
    "    n_iter_no_change=10,  \n",
    "    tol=1e-4 \n",
    ")\n",
    "\n",
    "# fit model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# predict and evaluate\n",
    "y_pred_gb_early_stop = gb_model.predict(X_test)\n",
    "print(\"Gradient Boosting Classifier Performance with Early Stopping:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "accuracy_gb_early_stop = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Accuracy: {accuracy_gb}\\n\")\n",
    "\n",
    "# retrieve the number of iterations the model used\n",
    "print(f\"Number of iterations used: {gb_model.n_estimators_}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics and plotting\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# predictions and probabilities for Random Forest\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc_rf = roc_auc_score(y_test, y_prob_rf)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
    "\n",
    "# predictions and probabilities for Gradient Boosting\n",
    "y_prob_gb = gb_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc_gb = roc_auc_score(y_test, y_prob_gb)\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_prob_gb)\n",
    "\n",
    "# plot ROC Curves for both models\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC = {roc_auc_rf:.2f})\")\n",
    "plt.plot(fpr_gb, tpr_gb, label=f\"Gradient Boosting (AUC = {roc_auc_gb:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label=\"Random Chance\")\n",
    "\n",
    "# graph details\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# predictions for Random Forest\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# predictions for Gradient Boosting\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "cm_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "# confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# random Forest Confusion Matrix\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=rf_model.classes_)\n",
    "disp_rf.plot(cmap=\"Blues\", ax=axes[0], colorbar=False)\n",
    "axes[0].set_title(\"Random Forest Confusion Matrix\")\n",
    "\n",
    "# gradient Boosting Confusion Matrix\n",
    "disp_gb = ConfusionMatrixDisplay(confusion_matrix=cm_gb, display_labels=gb_model.classes_)\n",
    "disp_gb.plot(cmap=\"Greens\", ax=axes[1], colorbar=False)\n",
    "axes[1].set_title(\"Gradient Boosting Confusion Matrix\")\n",
    "\n",
    "# layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
